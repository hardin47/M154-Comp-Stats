---
title: "Simulating"
author: "Jo Hardin"
subtitle: "September 23 + 25, 2024"
format: 
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    embed-resources: true
    multiplex: true
execute:
  echo: true
  warning: false
  message: false
bibliography: 
  - ../slides.bib
--- 


```{r include=FALSE}
library(tidyverse)
library(knitr)
library(lubridate)
library(openintro)
```


# Agenda  9/23/24

1. Why simulate?
2. What makes a good simulation?
3. Examples


## Goals of Simulating Complicated Models

The goal of simulating a complicated model is not only to create a program which will provide the desired results.  We also hope to be able to write code such that:

1. The problem is broken down into small pieces.
2. The problem has checks in it to see what works (run the lines *inside* the functions!).
3. uses simple code (as often as possible).


## Aside: a few R functions (`ifelse()`)
```{r}
data.frame(value = c(-2:2)) %>%
  mutate(abs_value = ifelse(value >=0, value, -value))  # abs val
```


## Aside: a few R functions (`case_when()`)
```{r}
set.seed(4747)
diamonds %>% select(carat, cut, color, price) %>%
  sample_n(20) %>%
  mutate(price_cat = case_when(
    price > 10000 ~ "expensive",
    price > 1500 ~ "medium", 
    TRUE ~ "inexpensive"))
```

## Aside: a few R functions (`sample()`)

*sampling*, *shuffling*,  and *resampling*: `sample()` 

```{r}
set.seed(47)
alph <- letters[1:10]
alph

sample(alph, 5, replace = FALSE) # sample (from a population)

sample(alph, 5, replace = TRUE) # sample (from a population)

sample(alph, 10, replace = FALSE)  # shuffle

sample(alph, 10, replace = TRUE)  # resample
```

## Why?

Three simulating methods are used for different purposes:

1. Monte Carlo methods - use repeated *sampling* from a **population** with known characteristics.


2. Randomization / Permutation methods - use *shuffling* (sampling without replacement from a **sample**) to test hypotheses of "no effect".


3. Bootstrap methods - use *resampling* (sampling with replacement from a **sample**) to establish confidence intervals.

## Aside: a few R functions (`set.seed()`)

What if we want to be able to generate the **same** random numbers (here on the interval from 0 to 1) over and over?

```{r}
set.seed(4747)
runif(4, 0, 1) # random uniform numbers

set.seed(123)
runif(4, 0, 1) # random uniform numbers

set.seed(4747)
runif(4, 0, 1) # random uniform numbers
```



# Small simulation example

```{=html}
<style>
.semi-transparent {
  opacity: 0.5;
}
</style>
```


* Simulation as easier than calculus for estimating probabilities.
* [Simulation to understand complicated models.]{.semi-transparent}
* [Simulation to consider methods with fewer assumptions.]{.semi-transparent}

## Sally & Joan

Consider a situation where Sally and Joan plan to meet to study in their college campus center [@mosteller1987;@MDSR]. They are both impatient people who will wait only 10 minutes for the other before leaving.

But their planning was incomplete. Sally said, "Meet me between 7 and 8 tonight at the student center." When should Joan plan to arrive at the campus center? And what is the probability that they actually meet?

## Simulate their meeting

Assume that Sally and Joan are both equally likely to arrive at the campus center anywhere between 7pm and 8pm. 

::: {.panel-tabset}

## vectorized 

```{r}
library(tictoc)
n <- 10000

tic()
meet_vect <- tibble(
  sally = runif(n, 0, max = 60),
  joan = runif(n, min = 0, max = 60),
  result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )
)
toc()
```
## function

```{r}

meet_func <- function(nada){
  tibble(
    sally = runif(1, min = 0, max = 60),
    joan = runif(1, min = 0, max = 60),
    result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )
  )
}
```


## `map()`

```{r}
tic()
meet_map <- 1:n |> 
  map(meet_func) |> 
  list_rbind()
toc()
```

## `for()` loop

```{r}
tic()
meet_for <- data.frame()
for(i in 1:n){
  meet_for <- meet_for |> rbind(
    tibble(
      sally = runif(1, min = 0, max = 60),
      joan = runif(1, min = 0, max = 60),
      result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )))
}
toc()

```

:::


##  Results

The results themselves are equivalent.  Differing values due to randomness in the simulation.

::: {.panel-tabset}

## vectorized

```{r}
meet_vect |> 
  group_by(result) |> 
  summarize(number = n()) |> 
  mutate(proprotion = number / sum(number))
```

## `map()`

```{r}

meet_map |> 
  group_by(result) |> 
  summarize(number = n())|> 
  mutate(proprotion = number / sum(number))
```

## `for()` loop

```{r}
meet_for |> 
  group_by(result) |> 
  summarize(number = n())|> 
  mutate(proprotion = number / sum(number))
```

:::

## Visualizing the meet up


```{r}
meet_map |> 
  ggplot(aes(x = joan, y = sally, color = result)) +
  geom_point(alpha = 0.3) + 
  geom_abline(intercept = 10, slope = 1) + 
  geom_abline(intercept = -10, slope = 1) + 
  scale_color_brewer(palette = "Paired")
```


## Simulation best practices

* no magic numbers
* comment your code
* use informative names
* set a seed for reproducibility


# Bigger simulation example

## Thoughts on simulating

* break down the problem into **small** steps
* check to see what works
* simple is better

## Roulette

* A roulette wheel has 38 slots numbered 00, 0, and 1â€“36.
Two are green, 18 are red, and 18 are black.
* If a gambler bets based on color, the return on a $1 bet
is $2
* A gambler has $20, and will continuously bet $1 on red
until they double their money (have $40) or lose the
money they came with
* What is the probability the gambler doubles their money?

Question: Without calculating probabilities, how could you
design an experiment to estimate this probability?

The example is taken directly from [Stat 279, Fall 2023](https://sta279-f23.github.io/) with Ciaran Evans. 

## Design the simulation

Step 1: Need a Roulette wheel!  ... and money  
Step 2: Will spin the wheel  
Step 3: Depending on the wheel, update the money.  
  * if spin is red: `money + 1`  
  * if spin is not red: `money - 1`  
Step 4: Keep spinning until `money = 40` or `money = 0`  
Step 5: Repeat many times  

## Step 1: create a roulette wheel

```{r}
wheel <- c(rep("green", 2), rep("black", 18), rep("red", 18))

wheel
```


* `rep()` repeats a value the specified number of times
* `c()` combines vectors (or scalars) into a single vector

## Step 2: spin the wheel

```{r}
spin <- sample(wheel, size = 1)

spin
```

## Step 3: update the money

```{r}
money <- 20

spin <- sample(wheel, size = 1)
money <- ifelse(spin == "red", money + 1, money  - 1)

spin
money
```




## Step 4: keep spinning (function first)

```{r}
spin_func <- function(money = 20, i){
  wheel <- c(rep("green", 2), rep("black", 18), rep("red", 18))
  spin <- sample(wheel, size = 1)
  money <- ifelse(spin == "red", money + 1, money  - 1)
  money
}

spin_func(10)
spin_func(spin_func(10))
```

## Aside: the `accumulate()` function

In R, the `accumulate()` function will run `f(f(f(f(x))))` as many times as asked.

```{r}
1:10 |> 
  accumulate(`+`)

1:10 |> 
  accumulate(`+`, .init = 47)
```

## Step 4: run multiple times

```{r}
set.seed(4747)
1:20 |> accumulate(spin_func, .init = 20)
```

## Step 4: another function

```{r}
game_func <- function(rep, money, iter){
  data.frame(interim_money = 1:iter |> accumulate(spin_func, .init = money),
             game = 0:iter,
             rep = rep)
}

game_func(1, 20, 3)
```


## Step 5: Repeat many times

Note that `map()` is specifically designed to work in vectorized ways, so it doesn't have an easy `while()` companion.  Which means that we need to make sure to `iter`ate long enough to either lose or double our money.

```{r}
output <- 1:20 |> map(game_func, money = 20, iter = 100000) |> 
  list_rbind()

output
```



## Step 6: Wrangle the output

```{r}
end <- output |> 
  group_by(rep) |> 
  filter(interim_money >= 40 | interim_money <= 0) |> 
  group_by(rep) |> 
  slice_min(game) |> 
  ungroup()

end
```

## Step 6: Wrangle the output

```{r}
end |> 
  summarize(prop_double = mean(interim_money == 40))
```


## Without magic numbers

```{r}
set.seed(4747)
money <- 10
games <- 20
iterations <- 10000


1:games |> map(game_func, money = money, iter = iterations) |> 
  list_rbind() |> 
  group_by(rep) |> 
  filter(interim_money >= 2 * money | interim_money <= 0) |> 
  group_by(rep) |> 
  slice_min(game) |> 
  ungroup() |> 
  summarize(prop_double = mean(interim_money == 2 * money))
```


# Modeling


* [Simulation as easier than calculus for estimating probabilities.]{.semi-transparent}
* Simulation to understand complicated models.
* [Simulation to consider methods with fewer assumptions.]{.semi-transparent}

## Bias in a model

```
talent ~ Normal (100, 15)
grades ~ Normal (talent, 15)
SAT ~ Normal (talent, 15)
```

College wants to admit students with 
> `talent > 115` 

... but they only have access to `grades` and `SAT` which are noisy estimates of `talent`.

The example is taken directly (and mostly verbatim) from a blog by Aaron Roth [Algorithmic Unfairness Without Any Bias Baked In](http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html).  

## Plan for accepting students

* Run a regression on a training dataset (`talent` is known for existing students)
* Find a model which predicts `talent` based on `grades` and `SAT`
* Choose students for whom predicted `talent` is above 115


##  Flaw in the plan ...

* there are two populations of students, the Reds and Blues. 
* Reds are the majority population (99%), and Blues are a small minority population (1%) 
* the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. 
* there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions

## What is really different?

> But there is one difference: the Blues have more money than the Reds, so they each take the **SAT twice**, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds.



##  Key aspect:

> The value of `SAT` means something different for the Reds versus the Blues

(They have different feature distributions.)


##  Let's see what happens ...

```{r echo=FALSE}
set.seed(470)
n_obs <- 100000
n.red <- n_obs*0.99
n.blue <- n_obs*0.01
reds <- rnorm(n.red, mean = 100, sd = 15)
blues <- rnorm(n.blue, mean = 100, sd = 15)

red.sat <- reds + rnorm(n.red, mean = 0, sd = 15)
blue.sat <- blues + 
    pmax(rnorm(n.blue, mean = 0, sd = 15),
         rnorm(n.blue, mean = 0, sd = 15))

red.grade <- reds + rnorm(n.red, mean = 0, sd = 15)
blue.grade <- blues + rnorm(n.blue, mean = 0, sd = 15)

college.data <- data.frame(talent = c(reds, blues),
                           SAT = c(red.sat, blue.sat),
                           grades = c(red.grade, blue.grade),
                           color = c(rep("red", n.red), rep("blue", n.blue)))

ggplot(college.data, aes(x = grades, y = SAT, color = color)) +
  geom_point(size = 0.5) +
  scale_color_identity(name = "Color Group",
                       guide = "legend") +
  geom_abline(intercept = 0, slope = 1)
  
```


## Two models:

```{r echo = FALSE}
red.lm = college.data %>%
  dplyr::filter(color == "red") %>%
  lm(talent ~ SAT + grades, data = .)

blue.lm = college.data %>%
  dplyr::filter(color == "blue") %>%
  lm(talent ~ SAT + grades, data = .)
```

Red model (SAT taken once):
```{r echo = FALSE}
red.lm %>% broom::tidy()
```
Blue model (SAT is max score of two):
```{r echo = FALSE}
blue.lm %>% broom::tidy()
```



## New data

* Generate new data and use the **two** models above, separately. 

* How well do the models predict if a student has `talent` > 115?

```{r echo=FALSE}
set.seed(470)
new.reds <- rnorm(n.red, mean = 100, sd = 15)
new.blues <- rnorm(n.blue, mean = 100, sd = 15)

new.red.sat <- new.reds + rnorm(n.red, mean = 0, sd = 15)
new.blue.sat <- new.blues + 
    pmax(rnorm(n.blue, mean = 0, sd = 15),
         rnorm(n.blue, mean = 0, sd = 15))

new.red.grade <- new.reds + rnorm(n.red, mean = 0, sd = 15)
new.blue.grade <- new.blues + rnorm(n.blue, mean = 0, sd = 15)

new.college.data <- data.frame(talent = c(new.reds, new.blues),
                           SAT = c(new.red.sat, new.blue.sat),
                           grades = c(new.red.grade, new.blue.grade),
                           color = c(rep("red", n.red), rep("blue", n.blue)))


new.red.pred <- new.college.data %>%
  filter(color == "red") %>%
  predict.lm(red.lm, newdata = .)

new.blue.pred <- new.college.data %>%
  filter(color == "blue") %>%
  predict.lm(blue.lm, newdata = .)

new.college.data <- new.college.data %>% 
  cbind(predicted = c(new.red.pred, new.blue.pred))

ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend")
```



## New data

:::: {.columns}

::: {.column width=50%}
```{r echo=FALSE}
ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend")
```
:::


::: {.column width=50%}
```{r echo=FALSE}
new.college.data <- new.college.data %>% 
  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & predicted < 115, 1, 0))

error.rates <- new.college.data %>% group_by(color) %>%
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```
:::

::::



## **TWO** models doesn't seem right????

What if we fit only one model to the entire dataset?

Afterall, there are laws against using protected classes to make decisions (housing, jobs, money loans, college, etc.)

```{r echo = FALSE}
global.lm = college.data %>%
  lm(talent ~ SAT + grades, data = .)

global.lm %>% broom::tidy()
```

(The coefficients kinda look like the red model...)


## How do the error rates change?

:::: {.columns}

::: {.column width=50%}
```{r echo=FALSE}
new.pred <- new.college.data %>%
  predict.lm(global.lm, newdata = .)

new.college.data <- new.college.data %>% 
  cbind(global.predicted = new.pred)

ggplot(new.college.data, aes(x = talent, 
                             y = global.predicted, 
                             color = color)) + 
  geom_point(size = 0.5) + 
  geom_hline(yintercept = 115) + 
  geom_vline(xintercept = 115) +
  scale_color_identity(name = "Color Group",
                       guide = "legend")
```
:::

::: {.column width=50%}
One model:
```{r echo = FALSE}
new.college.data.glb <- new.college.data %>% 
  mutate(fp = ifelse(talent < 115 & global.predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & global.predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & global.predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & global.predicted < 115, 1, 0))

error.rates <- new.college.data.glb %>% group_by(color) %>%
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```

Two separate models:
```{r echo = FALSE}
new.college.data.2mod <- new.college.data %>% 
  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),
         tp = ifelse(talent > 115 & predicted > 115, 1, 0),
         fn = ifelse(talent > 115 & predicted < 115, 1, 0),
         tn = ifelse(talent < 115 & predicted < 115, 1, 0))

error.rates <- new.college.data.2mod %>% group_by(color) %>%
  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),
            fpr = sum(fp) / (sum(fp) + sum(tn)),
            fnr = sum(fn) / (sum(fn) + sum(tp)),
            #fdr = sum(fp) / (sum(fp) + sum(tp)),
            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))

error.rates
```
:::
::::

##  What did we learn?

> with two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations

* depending on the nature of the distribution difference, it can be either to the benefit or the detriment of the minority population

* no explicit human bias, either on the part of the algorithm designer or the data gathering process

* the problem is exacerbated if we artificially force the algorithm to be group blind

* well intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time





## References


