---
title: "11. Unsupervised Learning"
description: |
  A quick dive into unsupervised methods.  We cover two clutering methods: hierarchical and partitioning (k-means and k-medoids). Additionally, we discuss latent Dirichlet allocation (LDA).
author:
  - name: Johanna Hardin
    url: https://m154-comp-stats.netlify.app/  
date: 2024-11-18
citation: false
embed-resources: true
execute: 
  message: false
  warning: false
image: ../../images/kmeans_5.jpg
---


```{r fig.cap = "Artwork by @allison_horst.", fig.alt = "Monsters as cluster centers moving around throughout the k-means algorithm.", preview = TRUE, echo = FALSE}
knitr::include_graphics("../../images/kmeans_5.jpg")
```


## Agenda <i class="fas fa-calendar-alt" target="_blank"></i>

### November 20, 2024

4. unsupervised methods

### November 25, 2024

1. distances
2. hierarchical clustering

### December 2, 2024

1. k-means clustering
2. k-medoids clustering

### December 4, 2024

1. latent Dirichlet allocation (LDA)

## Readings <i class="fas fa-book-open"></i> 

* Class notes: <a href = "https://st47s.com/Math154/Notes/09-clustering.html" target= "_blank">Unsupervised Methods</a>

* Gareth, Witten, Hastie, and Tibshirani (2021), <a href = "https://www.statlearning.com/" target = "_blank">Unsupervised Learning (Chapter 12) </a> Introduction to Statistical Learning.


## Reflection questions <i class="fas fa-lightbulb"></i>

* Why does the plot of within-cluster sum of squares vs. $k$ make an elbow-shape (hint: think about $k$ as it ranges from 1 to $n)?$

* How are the centers of the clusters in $k$-means calculated?  What about in $k$-medoids?

* Will a different initialization of the cluster centers always produce the same cluster output?

* How do distance metrics change a hierarchical clustering?  

* How can you choose $k$ with hierarchical clustering?

* What is the difference between single, average, and complete linkage in hierarchical clustering?

* What is the difference between agglomerative and divisive hierarchical clustering?

## Ethics considerations <i class="fas fa-balance-scale"></i> 

* What type of feature engineering is required for $k$-means / hierarchical clustering?

* How do you (can you?) know if your clustering has uncovered any real patterns in the data?

## Slides <i class="fas fa-desktop"></i> 

* <a href = "https://m154-comp-stats.netlify.app/slides/2024-11-18-cluster" target = "_blank">In class slides - clustering</a> for 11/25/24 + 12/2/24.

* <a href = "https://m154-comp-stats.netlify.app/slides/2024-12-04-lda" target = "_blank">In class slides - latent Dirichlet allocation</a> for 12/4/24.

* <a href = "https://m154-comp-stats.netlify.app/handout/ws18_m154_f24_hierarch.pdf" target = "_blank">WS 18 - hierarchical clustering</a>

* <a href = "https://m154-comp-stats.netlify.app/handout/ws19_m154_f24_kmeans.pdf" target = "_blank">WS 19 - k-means clustering</a>


## Additional Resources <i class="fas fa-laptop"></i> 

* <a href = "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/" target = "_blank">Fantastic k-means applet</a> by Naftali Harris.

* <a href = "http://varianceexplained.org/r/love-actually-network/" target = "_blank">Analyzing networks of characters in 'Love Actually'</a>



