---
title: "Using Recipes"
author: "Jo Hardin"
date: "October 21, 2021"
output:
  xaringan::moon_reader:
    nature:
      highlightlines: true
      titleSlideClass: ["right", "top", "my-title"]
---

```{r include=FALSE}
library(tidymodels)
knitr::opts_chunk$set(
  message=FALSE,
  warning=FALSE)
options(
  digits = 5,
  pillar.sigfig = 5
  )
```

```{r echo = FALSE}
#devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
xaringanExtra::use_panelset()
```

# Agenda 10/21/21

1. What needs to be done to the data?
2. `tidymodels` syntax for recipes
   a. recipe / feature engineering
   b. model
   c. workflow
   d. fit
   e. validate
3. Example

---

## Modeling

```{r}
library(tidymodels)
```


```{r echo = FALSE}
knitr::include_graphics("../images/process.png")
```

---

## Motivation

```{r echo = FALSE}
knitr::include_graphics("../images/garbage.png")
```
---


## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. (validate the model)


---

## partition the data

Put the testing data in your pocket (keep it secret from R!!)

```{r echo = FALSE, fig.cap = "Image credit: Julia Silge"}
knitr::include_graphics("../images/testtrain.png")
```
---

## partition the data


```{r}
library(tidymodels)
library(palmerpenguins)

set.seed(47)
penguin_split <- initial_split(penguins)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```

---
# build a recipe

1. Start the `recipe()`
2. Define the **variables** involved
3. Describe preprocessing **step-by-step**

---

# feature engineering

> feature engineering is the process of transforming raw data into features (variables) that are better predictors (for the model at hand) of the 

* create new variables (e.g., combine levels -> from state to region)
* transform variable (e.g., log, polar coordinates)
* continuous variables -> discrete (e.g., binning)
* numerical categorical data -> factors / character strings (one hot encoding)
* time -> discretized time
* missing values -> imputed
* NA -> level
* continuous variables -> center & scale ("normalize")



---


# `step_` functions

For more information: https://recipes.tidymodels.org/reference/index.html

```{r}
apropos("^step_")
```

---

## the data: penguins

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill", fig.align='right', out.width="30%"}
knitr::include_graphics("../images/penguins.png")
```

```{r}
penguins
```


---


## recipe

```{r}
penguin_recipe <-
  recipe(body_mass_g ~ species + island + bill_length_mm + 
           bill_depth_mm + flipper_length_mm + sex + year,
         data = penguin_train) %>%
  step_mutate(year = as.factor(year)) %>%
  step_unknown(sex, new_level = "unknown") %>%
  step_relevel(sex, ref_level = "female") %>%
  update_role(island, new_role = "id variable")
```


```{r}
summary(penguin_recipe)
```



---

## model

To specify a model:

1. pick a **model**
2. set the **mode** (regression vs classification, if needed)
3. set the **engine**

```{r}
penguin_lm <- linear_reg() %>%
  set_engine("lm")
```

```{r}
penguin_lm
```

---
### engines

```{r}
show_engines("nearest_neighbor")
show_engines("decision_tree")
```

---
### engines

```{r}
show_engines("rand_forest")
```

---
### engines

```{r}
show_engines("svm_poly")
show_engines("svm_rbf")
```

---
### engines

```{r}
show_engines("linear_reg")
```

---

## workflow

```{r}
penguin_wflow <- workflow() %>%
  add_model(penguin_lm) %>%
  add_recipe(penguin_recipe)
```

```{r}
penguin_wflow
```


---

## fit


```{r}
penguin_fit <- penguin_wflow %>%
  fit(data = penguin_train)
```


```{r}
penguin_fit %>% tidy()
```

---

## model parameters

* Some model parameters are tuned from the data (some aren't).
  - linear model coefficients are optimized (not tuned)
  - k-nn value of "k" is tuned

* If the model is tuned using the data, the same data **cannot** be used to assess the model.

* With Cross Validation, you iteratively put data in your pocket.

* For example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide2.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide3.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide4.png")
```


---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide5.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide6.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide7.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide8.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide9.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide10.png")
```

---
## Cross validation
### for tuning parameters

```{r echo = FALSE, fig.cap = "Image credit: Alison Hill"}
knitr::include_graphics("../images/CV/Slide11.png")
```



---

## model parameters

* Some model parameters are tuned from the data (some aren't).
  - linear model coefficients are optimized (not tuned)
  - k-nn value of "k" is tuned

* If the model is tuned using the data, the same data **cannot** be used to assess the model.

* With Cross Validation, you iteratively put data in your pocket.

* For example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.
