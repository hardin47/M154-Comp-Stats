<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Support Vector Machines</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jo Hardin" />
    <script src="2021-11-09-svm_files/header-attrs-2.11.3/header-attrs.js"></script>
    <link href="2021-11-09-svm_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="2021-11-09-svm_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="2021-11-09-svm_files/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="2021-11-09-svm_files/panelset-0.2.6/panelset.js"></script>
  </head>
  <body>
    <textarea id="source">
class: right, top, my-title, title-slide

# Support Vector Machines
### Jo Hardin
### November 9 &amp; 11, 2021

---






# Agenda 11/09/21

1. linearly separable
2. dot products
3. support vector formulation

---

## `tidymodels` syntax

1. partition the data
2. build a recipe
3. select a model
4. create a workflow
5. fit the model  
6. validate the model

---

## Support Vector Machines

&gt; SVMs create both linear and non-linear decision boundaries.  They are incredibly efficient because of the **kernel trick** which allows the computation to be done in a high dimension.

---

## Deriving SVM formulation

`\(\rightarrow\)` see class notes for all technical details

* Mathematics of the optimization to find the widest linear boundary in a space where the two groups are completely separable.

* Note from derivation: both the optimization and the application are based on dot products.

* Transform the data to a higher space so that the points are linearly separable.  Perform SVM in that space.

* Recognize that "performing SVM in higher space" is exactly equivalent to using a kernel in the original dimension.

* Allow for points to cross the boundary using soft margins.

---

# Agenda 11/11/21

1. not linearly separable (SVM)
2. kernels (SVM)
3. support vector formulation

---

******
**Algorithm**:   Support Vector Machine

******
1. Using cross validation, find values of `\(C, \gamma, d, r\)`, etc.  (and the kernel function!)
2. Using Lagrange multipliers (read: the computer), solve for `\(\alpha_i\)` and `\(b\)`.
3.  Classify an unknown observation (${\bf u}$) as "positive" if:
`$$\sum \alpha_i y_i \phi({\bf x}_i) \cdot \phi({\bf u}) + b  = \sum \alpha_i y_i K({\bf x}_i, {\bf u}) + b \geq 0$$`

******

---
---

## SVM example w defaults



.panelset[

.panel[.panel-name[recipe]

```r
penguin_svm_recipe &lt;-
  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +
           body_mass_g, data = penguin_train) %&gt;%
  step_normalize(all_predictors())

summary(penguin_svm_recipe)
```

```
## # A tibble: 5 × 4
##   variable          type    role      source  
##   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
## 1 bill_length_mm    numeric predictor original
## 2 bill_depth_mm     numeric predictor original
## 3 flipper_length_mm numeric predictor original
## 4 body_mass_g       numeric predictor original
## 5 sex               nominal outcome   original
```
]

.panel[.panel-name[model]

```r
penguin_svm_lin &lt;- svm_linear() %&gt;%
  set_engine("LiblineaR") %&gt;%
  set_mode("classification")

penguin_svm_lin
```

```
## Linear Support Vector Machine Specification (classification)
## 
## Computational engine: LiblineaR
```
]

.panel[.panel-name[workflow]


```r
penguin_svm_lin_wflow &lt;- workflow() %&gt;%
  add_model(penguin_svm_lin) %&gt;%
  add_recipe(penguin_svm_recipe)

penguin_svm_lin_wflow
```

```
## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: svm_linear()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Linear Support Vector Machine Specification (classification)
## 
## Computational engine: LiblineaR
```
]

.panel[.panel-name[fit]
.pull-left[

```r
penguin_svm_lin_fit &lt;- 
  penguin_svm_lin_wflow %&gt;%
  fit(data = penguin_train)

penguin_svm_lin_fit 
```
]
.pull-right[

```
## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: svm_linear()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## $TypeDetail
## [1] "L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)"
## 
## $Type
## [1] 1
## 
## $W
##      bill_length_mm bill_depth_mm flipper_length_mm body_mass_g     Bias
## [1,]        0.24891        1.0802          -0.22564      1.3284 0.069927
## 
## $Bias
## [1] 1
## 
## $ClassNames
## [1] male   female
## Levels: female male
## 
## $NbClass
## [1] 2
## 
## attr(,"class")
## [1] "LiblineaR"
```
]

]


]

---

#### Fit again

```
## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: svm_linear()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## $TypeDetail
## [1] "L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)"
## 
## $Type
## [1] 1
## 
## $W
##      bill_length_mm bill_depth_mm flipper_length_mm body_mass_g     Bias
## [1,]        0.24891        1.0802          -0.22564      1.3284 0.069927
## 
## $Bias
## [1] 1
## 
## $ClassNames
## [1] male   female
## Levels: female male
## 
## $NbClass
## [1] 2
## 
## attr(,"class")
## [1] "LiblineaR"
```

---

## SVM example w CV tuning (RBF kernel)

.panelset[

.panel[.panel-name[recipe]

```r
penguin_svm_recipe &lt;-
  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +
           body_mass_g, data = penguin_train) %&gt;%
  step_normalize(all_predictors())

summary(penguin_svm_recipe)
```

```
## # A tibble: 5 × 4
##   variable          type    role      source  
##   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
## 1 bill_length_mm    numeric predictor original
## 2 bill_depth_mm     numeric predictor original
## 3 flipper_length_mm numeric predictor original
## 4 body_mass_g       numeric predictor original
## 5 sex               nominal outcome   original
```
]

.panel[.panel-name[model]

```r
penguin_svm_rbf &lt;- svm_rbf(cost = tune(),
                           rbf_sigma = tune()) %&gt;%
  set_engine("kernlab") %&gt;%
  set_mode("classification")

penguin_svm_rbf
```

```
## Radial Basis Function Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = tune()
##   rbf_sigma = tune()
## 
## Computational engine: kernlab
```
]

.panel[.panel-name[workflow]


```r
penguin_svm_rbf_wflow &lt;- workflow() %&gt;%
  add_model(penguin_svm_rbf) %&gt;%
  add_recipe(penguin_svm_recipe)

penguin_svm_rbf_wflow
```

```
## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: svm_rbf()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Radial Basis Function Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = tune()
##   rbf_sigma = tune()
## 
## Computational engine: kernlab
```
]

.panel[.panel-name[CV]

```r
set.seed(234)
penguin_folds &lt;- vfold_cv(penguin_train,
                          v = 4)
```
]

.panel[.panel-name[param]

```r
# the tuned parameters also have default values you can use
penguin_grid &lt;- grid_regular(cost(),
                             rbf_sigma(),
                             levels = 8)

penguin_grid
```

```
## # A tibble: 64 × 2
##           cost  rbf_sigma
##          &lt;dbl&gt;      &lt;dbl&gt;
##  1  0.00097656 1     e-10
##  2  0.0043128  1     e-10
##  3  0.019047   1     e-10
##  4  0.084119   1     e-10
##  5  0.37150    1     e-10
##  6  1.6407     1     e-10
##  7  7.2458     1     e-10
##  8 32          1     e-10
##  9  0.00097656 2.6827e- 9
## 10  0.0043128  2.6827e- 9
## # … with 54 more rows
```
]

.panel[.panel-name[tune]

```r
# this takes a few minutes
penguin_svm_rbf_tune &lt;- 
  penguin_svm_rbf_wflow %&gt;%
  tune_grid(resamples = penguin_folds,
            grid = penguin_grid)

penguin_svm_rbf_tune 
```

```
## # Tuning results
## # 4-fold cross-validation 
## # A tibble: 4 × 4
##   splits           id    .metrics           .notes          
##   &lt;list&gt;           &lt;chr&gt; &lt;list&gt;             &lt;list&gt;          
## 1 &lt;split [186/63]&gt; Fold1 &lt;tibble [128 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 2 &lt;split [187/62]&gt; Fold2 &lt;tibble [128 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 3 &lt;split [187/62]&gt; Fold3 &lt;tibble [128 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 4 &lt;split [187/62]&gt; Fold4 &lt;tibble [128 × 6]&gt; &lt;tibble [0 × 1]&gt;
```

]


]

---
##  SVM model output


```r
penguin_svm_rbf_tune %&gt;%
  collect_metrics() %&gt;%
  filter(.metric == "accuracy") %&gt;%
  ggplot() + 
  geom_line(aes(color = as.factor(cost), y = mean, x = rbf_sigma)) +
  geom_point(aes(color = as.factor(cost), y = mean, x = rbf_sigma)) +
  labs(color = "Cost") +
  scale_x_continuous(trans='log10')
```

![](2021-11-09-svm_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;



---

## SVM model output - take two


```r
penguin_svm_rbf_tune %&gt;%
  collect_metrics() %&gt;%
  filter(.metric == "accuracy") %&gt;%
  ggplot() + 
  geom_line(aes(color = as.factor(rbf_sigma), y = mean, x = cost)) +
  geom_point(aes(color = as.factor(rbf_sigma), y = mean, x = cost)) +
  labs(color = "Cost") +
  scale_x_continuous(trans='log10')
```

![](2021-11-09-svm_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;




---

## SVM Final model


```r
penguin_svm_rbf_best &lt;- finalize_model(
  penguin_svm_rbf,
  select_best(penguin_svm_rbf_tune, "accuracy"))

penguin_svm_rbf_best
```

```
## Radial Basis Function Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = 0.371498572284237
##   rbf_sigma = 1
## 
## Computational engine: kernlab
```

```r
penguin_svm_rbf_final &lt;-
  workflow() %&gt;%
  add_model(penguin_svm_rbf_best) %&gt;%
  add_recipe(penguin_svm_recipe) %&gt;%
  fit(data = penguin_train)
```

---

## SVM Final model


```r
penguin_svm_rbf_final
```

```
## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: svm_rbf()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 0.371498572284237 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  1 
## 
## Number of Support Vectors : 137 
## 
## Objective Function Value : -31.8 
## Training error : 0.052209 
## Probability model included.
```

---

## Test predictions


```r
penguin_svm_rbf_final %&gt;%
  predict(new_data = penguin_test) %&gt;%
  cbind(penguin_test) %&gt;%
  select(sex, .pred_class) %&gt;%
  table()
```

```
##         .pred_class
## sex      female male
##   female     39    5
##   male        4   36
```


```r
penguin_svm_rbf_final %&gt;%
  predict(new_data = penguin_test) %&gt;%
  cbind(penguin_test) %&gt;%
  conf_mat(sex, .pred_class)
```

```
##           Truth
## Prediction female male
##     female     39    4
##     male        5   36
```

---

## Other measures


```r
# https://yardstick.tidymodels.org/articles/metric-types.html
class_metrics &lt;- metric_set(accuracy, sensitivity, 
                            specificity, f_meas)

penguin_svm_rbf_final %&gt;%
  predict(new_data = penguin_test) %&gt;%
  cbind(penguin_test) %&gt;%
  class_metrics(truth = sex, estimate = .pred_class)
```

```
## # A tibble: 4 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary       0.89286
## 2 sens     binary       0.88636
## 3 spec     binary       0.9    
## 4 f_meas   binary       0.89655
```


---

## Bias-Variance Tradeoff

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="../images/varbias.png" alt="Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  image credit: ISLR" width="90%" /&gt;
&lt;p class="caption"&gt;Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  image credit: ISLR&lt;/p&gt;
&lt;/div&gt;

---

## Reflecting on Model Building

&lt;div class="figure"&gt;
&lt;img src="../images/modelbuild1.png" alt="Image credit: https://www.tmwr.org/" width="2176" /&gt;
&lt;p class="caption"&gt;Image credit: https://www.tmwr.org/&lt;/p&gt;
&lt;/div&gt;

---

## Reflecting on Model Building

&lt;div class="figure"&gt;
&lt;img src="../images/modelbuild2.png" alt="Image credit: https://www.tmwr.org/" width="2067" /&gt;
&lt;p class="caption"&gt;Image credit: https://www.tmwr.org/&lt;/p&gt;
&lt;/div&gt;



---

## Reflecting on Model Building

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="../images/modelbuild3.png" alt="Image credit: https://www.tmwr.org/" width="70%" /&gt;
&lt;p class="caption"&gt;Image credit: https://www.tmwr.org/&lt;/p&gt;
&lt;/div&gt;


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightlines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
